{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TellTales_MobilenetV2_multilabel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goergevo/FreeRTOSxMegaPortAndDemo/blob/master/TellTales_MobilenetV2_multilabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC-mUVF5rKRh",
        "colab_type": "text"
      },
      "source": [
        "MobileNet_V2 **multilabel** transfer learning.\n",
        "Test with CPU or GPU.\n",
        "\n",
        "To keep the backend running:\n",
        "in Chrome: alt-cmd I\n",
        "and insert this code:\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRsdI3HIOdEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q livelossplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjrfc20joAKe",
        "colab_type": "text"
      },
      "source": [
        "How much memory does my GPU have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtoc-AopnPWz",
        "colab_type": "code",
        "outputId": "1341e4a1-4a1e-4c66-916f-cb6549177123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "try:\n",
        "  gpu = GPUs[0]\n",
        "  def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "  printm()\n",
        "except:\n",
        "  print(\"no GPU!\")\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "no GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuKr6n9WO8Mi",
        "colab_type": "code",
        "outputId": "74aa2e69-7549-4fa7-d5a3-9cbfe40a51d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download and install the TensorFlow 2.0\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  print(\"no Tensorflow 2.0 available?\")\n",
        "  pass"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ENlaZPSO_sQ",
        "colab_type": "code",
        "outputId": "e5ea299f-98d7-4e16-af61-9501dde2ad38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount my personal google drive\n",
        "from google.colab import drive\n",
        "# mount under this path in the local runtime:\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVl0tUe0gD4",
        "colab_type": "code",
        "outputId": "c67a706d-f55b-4d35-ced5-aba9bb949dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!jupyter nbconvert --to script \"drive/My Drive/Colab Notebooks/TellTales_MobilenetV2_multilabel.ipynb\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/pkg_resources/py2_warn.py:19: UserWarning: ************************************************************\n",
            "You are running Setuptools on Python 2, which is no longer\n",
            "supported and\n",
            ">>> SETUPTOOLS WILL STOP WORKING <<<\n",
            "in a subsequent release. Please ensure you are installing\n",
            "Setuptools using pip 9.x or later or pin to `setuptools<45`\n",
            "in your environment.\n",
            "If you have done those things and are still encountering\n",
            "this message, please comment in\n",
            "https://github.com/pypa/setuptools/issues/1458\n",
            "about the steps that led to this unsupported combination.\n",
            "************************************************************\n",
            "  sys.version_info < (3,) and warnings.warn(\"*\" * 60 + msg + \"*\" * 60)\n",
            "[NbConvertApp] Converting notebook drive/My Drive/Colab Notebooks/TellTales_MobilenetV2_multilabel.ipynb to script\n",
            "[NbConvertApp] Writing 31790 bytes to drive/My Drive/Colab Notebooks/TellTales_MobilenetV2_multilabel.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FphIWF-mPDrw",
        "colab_type": "code",
        "outputId": "18857c47-1bbd-4f88-91b7-cafb431659ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive\"\n",
        "!cp \"/content/drive/My Drive/imagesRedGreen2_1000_128x128.zip\" .\n",
        "#!rm -r imagesRedGreen200\n",
        "#!rm -r imagesRedGreen1000\n",
        "!unzip -q -u imagesRedGreen2_1000_128x128.zip\n",
        "!ls -l"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'\n",
            "'How to get started with Drive.pdf'\n",
            " imagesRedGreen1000_112x112.zip\n",
            " imagesRedGreen200_112x112.zip\n",
            " imagesRedGreen20_112x112.zip\n",
            " imagesRedGreen2_1000_128x128.zip\n",
            " traindir_20191220_170054_gut_97prozent\n",
            " traindir_20191220_232930_gut_98prozent\n",
            " traindir_20191222_173741_gut_99prozent\n",
            " traindir_20191223_172537_alpha1.0_gut_99.9prozent\n",
            " traindir_20191224_223926_alpha0.5_gut_99.5prozent\n",
            " traindir_20191225_074057_alpha0.35_gut_98.9prozent\n",
            " traindir_20191226_144336_alpha0.35_99prozent\n",
            " traindir_20200129_234140_alpha0.35_97.55prozent\n",
            " traindir_20200129_235150_alpha1.0_98.42prozent\n",
            " traindir_20200130_123444_aplha1.0_99.35prozent\n",
            " traindir_20200131_121503\n",
            "total 146960\n",
            "-rw-r--r-- 1 root root   5102824 Jan 31 21:41 df_train.csv\n",
            "-rw-r--r-- 1 root root   1007665 Jan 31 21:41 df_valid.csv\n",
            "drwx------ 4 root root      4096 Jan 31 21:37 drive\n",
            "drwxr-xr-x 4 root root      4096 Jan 31 17:09 imagesRedGreen2_1000_128x128\n",
            "-rw------- 1 root root 144342334 Jan 31 22:00 imagesRedGreen2_1000_128x128.zip\n",
            "drwxr-xr-x 3 root root      4096 Jan 31 21:41 logs\n",
            "drwxrwxr-x 3 root root      4096 Jan 31 17:12 __MACOSX\n",
            "drwxr-xr-x 1 root root      4096 Jan 13 16:38 sample_data\n",
            "drwxr-xr-x 2 root root      4096 Jan 31 21:38 traindir_20200131_213834\n",
            "drwxr-xr-x 2 root root      4096 Jan 31 21:54 traindir_20200131_215429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcJ_Hoz8PGyp",
        "colab_type": "code",
        "outputId": "e2a59117-b413-436b-8fc8-77bf52c760ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#  ***Multilabel***\n",
        "#  Transfer Learning with MobileNet_V2\n",
        "# tutorial: https://www.tensorflow.org/tutorials/images/transfer_learning\n",
        "#\n",
        "# https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
        "# https://medium.com/@ckyeungac/a-simple-trick-about-multi-label-image-classification-with-imagedatagenerator-in-keras-4ab95364a481\n",
        "# https://github.com/aditya9898/transfer-learning/blob/master/transfer-learning.py\n",
        "#\n",
        "# The building of a model is a 3 step process:\n",
        "#\n",
        "#    Importing the pre-trained model and adding the dense layers.\n",
        "#    Loading train data into ImageDataGenerators.\n",
        "#    Training and Evaluating model.\n",
        "#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import glob\n",
        "import pickle\n",
        "import datetime\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras  # we are using keras which is part of tensorflow 2.0\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "# more info on callbacks: https://keras.io/callbacks/ model saver is cool too.\n",
        "\n",
        "print(\"Keras Version=\" + keras.__version__)  # expecting 2.2.4-tf\n",
        "print(\"Tensorflow Version=\" + tf.__version__)  # expecting Tensorflow version=2.0.0\n",
        "\n",
        "NAME = \"TellTales-CNN\"\n",
        "\n",
        "# The helper function\n",
        "def multilabel_flow_from_dataframe(data_generator, mlb, df):\n",
        "    for x, y in data_generator:\n",
        "        # num_samples = len(y)\n",
        "        indices = y.astype(np.int).tolist()\n",
        "        y_multi = mlb.transform(df.iloc[indices]['tags'].values.tolist())\n",
        "        yield x, y_multi\n",
        "\n",
        "\n",
        "def createDataFrame_fromDirectory(images_root_dir):\n",
        "    df = pd.DataFrame({'imagepath': [], 'tags': [], 'index': []})\n",
        "    i = int(0)\n",
        "    for filename in images_root_dir.rglob('*/*.jpg'):\n",
        "        # print(filename)\n",
        "        # the directory name contains the angle combinations: .../degree_45_15/xy.png\n",
        "        labels = filename.parts[-2].split(\"_\")\n",
        "        df = df.append({'imagepath': str(filename), 'tags': labels, 'index': i}, ignore_index=True)\n",
        "        i += 1\n",
        "    #print(df)\n",
        "    return df\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keras Version=2.2.4-tf\n",
            "Tensorflow Version=2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta3rywgTQj1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ============================================================================\n",
        "#\n",
        "class trainMobileNetV2_MultiLabel:\n",
        "    def __init__(self):\n",
        "        self.width = 0\n",
        "        self.height = 0\n",
        "        self.model_data_name = \"\"\n",
        "        self.mlb_data_name = \"\"\n",
        "        self.numclasses = 0\n",
        "        self.model_plot_name = 'model.png'\n",
        "        self.number_of_epochs = 0\n",
        "        self.model = 0\n",
        "        self.history = 0\n",
        "        self.root_dir_train = 0\n",
        "        self.root_dir_valid = 0\n",
        "        self.df_train = 0\n",
        "        self.df_valid = 0\n",
        "        self.mlb_train = 0\n",
        "        self.mlb_valid = 0\n",
        "        self.classes_list = 0\n",
        "\n",
        "    # ============================================================\n",
        "    #\n",
        "    def enumerate_training_images(self):\n",
        "        # parse the images directories and create the dataframe with images pathes and label lists\n",
        "        self.df_train = createDataFrame_fromDirectory(self.root_dir_train)\n",
        "        self.df_valid = createDataFrame_fromDirectory(self.root_dir_valid)\n",
        "\n",
        "        self.df_train.to_csv(\"df_train.csv\", index=False)\n",
        "        self.df_valid.to_csv(\"df_valid.csv\", index=False)\n",
        "\n",
        "        # Fit a MultiLabelBinarizer\n",
        "        self.mlb_train = MultiLabelBinarizer()\n",
        "        self.mlb_train.fit(self.df_train['tags'].values.tolist())\n",
        "        self.mlb_valid = MultiLabelBinarizer()\n",
        "        self.mlb_valid.fit(self.df_valid['tags'].values.tolist())\n",
        "\n",
        "        # use this later for the predictions\n",
        "        self.classes_list = self.mlb_train.classes_\n",
        "        self.numclasses = len(self.classes_list)\n",
        "        print(\"found this image classes in training data:\")\n",
        "        print(self.classes_list)\n",
        "\n",
        "    # ============================================================\n",
        "    #\n",
        "    def build_model(self, num_fix_layers, learning_rate, alpha_param):\n",
        "        # We import the MobileNet model without its last layer ...\n",
        "        # weights are downloaded and cached here: ~/.keras/models/mobilenet_1_0_224_tf_no_top.h5\n",
        "        #\n",
        "        # https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py\n",
        "        # The paper demonstrates the performance of MobileNets using `alpha` values of\n",
        "        # 1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
        "        #\n",
        "        # For each of these `alpha` values, weights for 5 different input image sizes\n",
        "        # are provided (224, 192, 160, 128, and 96).\n",
        "        #\n",
        "        base_model = keras.applications.MobileNetV2(input_shape=(self.width, self.height, 3),\n",
        "                                                    alpha= alpha_param,\n",
        "                                                    #weights='imagenet',# imports the trained mobilenet weights\n",
        "                                                    include_top=False)  # and discards the last 1000 neuron layer.\n",
        "\n",
        "        #base_model.trainable = False\n",
        "\n",
        "        # ... and add a few dense layers so that our model can learn more complex functions. The dense layers must\n",
        "        # have the relu activation function and the last layer,which contains as many neurons as the number of\n",
        "        # classes must have the softmax activation.\n",
        "        x = base_model.output\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        # we add dense layers so that the model can learn more complex functions and\n",
        "        # classify for better results.\n",
        "        #todo: make it a parameter\n",
        "        #x = keras.layers.Dense(1280, activation='relu')(x)\n",
        "        #x = keras.layers.Dense(1280, activation='relu')(x)\n",
        "        x = keras.layers.Dense(512, activation='relu')(x)\n",
        "        # final layer with sigmoid activation for multilabel classification\n",
        "        predictions = keras.layers.Dense(self.numclasses, activation='sigmoid')(x)\n",
        "\n",
        "        # Next we make a model based on the architecture we have provided.\n",
        "        # specify the inputs\n",
        "        # specify the outputs\n",
        "        self.model = keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
        "        # now a model has been created based on our architecture\n",
        "\n",
        "        # Now that we have our model, as we will be using the pre-trained weights, that our model has been trained on (\n",
        "        # imagenet dataset), we have to set all the weights to be non-trainable. We will only be training the last Dense\n",
        "        # layers that we have made previously.\n",
        "\n",
        "        for layer in self.model.layers[:num_fix_layers]:\n",
        "            layer.trainable = False\n",
        "        for layer in self.model.layers[num_fix_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "        # or: all layers non-trainable:\n",
        "        # for layer in model.layers:\n",
        "        #    layer.trainable=False\n",
        "\n",
        "        # To check the architecture of our model, we simply need to use this line of code given below.\n",
        "        #for i, layer in enumerate(self.model.layers):\n",
        "        #    print(i, layer.name, layer.trainable)\n",
        "        \n",
        "        # another method to check the model structure:\n",
        "        #self.model.summary()\n",
        "\n",
        "        # For this we first compile the model that we made, and then train our model with our generator:\n",
        "        #   Adam optimizer\n",
        "        #   loss function will be binary crossentropy for multi-label classification\n",
        "        #   evaluation metric will be accuracy\n",
        "        opt = keras.optimizers.Adam(lr=learning_rate)\n",
        "        # todo: learning rate decay\n",
        "        # opt = keras.optimizers.Adam(lr=learning_rate, decay=learning_rate / number_of_epochs)\n",
        "        self.model.compile(optimizer=opt,\n",
        "                           loss='binary_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "        # create a graphical plot of the model\n",
        "        #  keras.utils.plot_model(self.model, to_file=self.model_plot_name)\n",
        "\n",
        "    # ============================================================\n",
        "    #\n",
        "    def train(self, number_of_epochs, batch_size):\n",
        "\n",
        "        train_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input)\n",
        "        train_generator = train_datagen.flow_from_dataframe(\n",
        "            dataframe=self.df_train,\n",
        "            directory='./',\n",
        "            x_col='imagepath',\n",
        "            y_col='index',\n",
        "            class_mode='other',\n",
        "            target_size=(self.width, self.height),\n",
        "            color_mode=\"rgb\",\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        multilabel_train_generator = multilabel_flow_from_dataframe(train_generator, self.mlb_train, self.df_train)\n",
        "\n",
        "        # validate the model with unseen images\n",
        "        valid_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input)\n",
        "        valid_generator = valid_datagen.flow_from_dataframe(\n",
        "            dataframe=self.df_valid,\n",
        "            directory='./',\n",
        "            x_col='imagepath',  # column names in the df\n",
        "            y_col='index',\n",
        "            class_mode='other',\n",
        "            target_size=(self.width, self.height),\n",
        "            color_mode=\"rgb\",\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        multilabel_valid_generator = multilabel_flow_from_dataframe(valid_generator, self.mlb_valid, self.df_valid)\n",
        "\n",
        "        # log decreasing learning rate for Tensorboard\n",
        "        #\n",
        "        logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
        "        file_writer.set_as_default()\n",
        "        def lr_schedule(epoch, lr):\n",
        "            \"\"\"\n",
        "            Returns a custom learning rate that decreases as epochs progress.\n",
        "            \"\"\"\n",
        "            learning_rate = lr\n",
        "            #learning_rate = 0.01\n",
        "            if epoch > 5:\n",
        "                learning_rate /= 2\n",
        "            if epoch > 8:\n",
        "                learning_rate /= 4\n",
        "            if epoch > 11:\n",
        "                learning_rate /= 8\n",
        "\n",
        "            tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
        "            return learning_rate\n",
        "\n",
        "        plot_losses = PlotLossesCallback()\n",
        "        #logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "        #tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "        lr_callback        = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "        step_size_train = train_generator.n // train_generator.batch_size\n",
        "        step_size_valid = valid_generator.n // valid_generator.batch_size\n",
        "        self.number_of_epochs = number_of_epochs\n",
        "        history = self.model.fit_generator( generator=multilabel_train_generator,\n",
        "                                  steps_per_epoch=step_size_train,\n",
        "                                  epochs=number_of_epochs,\n",
        "                                  validation_data=multilabel_valid_generator,\n",
        "                                  validation_steps=step_size_valid,\n",
        "                                  # total number of validation batches to perform on an epoch\n",
        "                                  validation_freq=1,\n",
        "                                  # how many training epochs to run before a new validation run is performed\n",
        "                                  callbacks=[plot_losses, lr_callback],\n",
        "                                  #callbacks=[tensorboard_callback, lr_callback],\n",
        "                                  verbose=1)  # 0=silent, 1 = progress bar, 2 = one line per epoch\n",
        "        # Now we have a trained model.\n",
        "        self.history = history\n",
        "\n",
        "        print('===== history data ======')\n",
        "        print(history.history.keys())\n",
        "        print('=========================')\n",
        "\n",
        "        # This saves the model exactly in the state where it was stopped.\n",
        "        # This model can be used to continue training.\n",
        "        self.model.save(str(self.model_data_name))\n",
        "\n",
        "        # save the multi-label binarizer to disk\n",
        "        # todo: save mlb_train as well?\n",
        "        print(\"[INFO] serializing label binarizer...\")\n",
        "        f = open(self.mlb_data_name, \"wb\")\n",
        "        f.write(pickle.dumps(self.mlb_valid))\n",
        "        f.close()\n",
        "\n",
        "        print(\"Learning done.\")\n",
        "\n",
        "    # =================================\n",
        "    # plot accuracy and loss\n",
        "    #\n",
        "    def plotAccuracy(self):\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
        "        f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "        epoch_list = list(range(1, self.number_of_epochs + 1))\n",
        "        ax1.plot(epoch_list, self.history.history['accuracy'], label='Train Accuracy')\n",
        "        ax1.plot(epoch_list, self.history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        ax1.set_xticks(np.arange(0, self.number_of_epochs + 1, 5))\n",
        "        ax1.set_ylabel('Accuracy Value')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_title('Accuracy')\n",
        "        l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "        ax2.plot(epoch_list, self.history.history['loss'], label='Train Loss')\n",
        "        ax2.plot(epoch_list, self.history.history['val_loss'], label='Validation Loss')\n",
        "        ax2.set_xticks(np.arange(0, self.number_of_epochs + 1, 5))\n",
        "        ax2.set_ylabel('Loss Value')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_title('Loss')\n",
        "        l2 = ax2.legend(loc=\"best\")\n",
        "        plt.show()\n",
        "\n",
        "        k = cv2.waitKey(0) & 0xFF\n",
        "        if k == 27:  # wait for ESC key to exit\n",
        "            cv2.destroyAllWindows()\n",
        "        elif k == ord('s'):  # wait for 's' key to save and exit\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "    # =================================\n",
        "    # plot accuracy and loss\n",
        "    #\n",
        "    def plot2(self, traindatadir):\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
        "        f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "        epoch_list = list(range(1, self.number_of_epochs + 1))\n",
        "        # ax1.plot(epoch_list, self.history.history['acc'], label='Train Accuracy')\n",
        "        # ax1.plot(epoch_list, self.history.history['val_acc'], label='Validation Accuracy')\n",
        "        ax1.plot(epoch_list, self.history.history['accuracy'], label='Train Accuracy')\n",
        "        ax1.plot(epoch_list, self.history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        ax1.set_xticks(np.arange(0, self.number_of_epochs + 1, 5))\n",
        "        ax1.set_ylabel('Accuracy Value')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_title('Accuracy')\n",
        "        l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "        ax2.plot(epoch_list, self.history.history['loss'], label='Train Loss')\n",
        "        ax2.plot(epoch_list, self.history.history['val_loss'], label='Validation Loss')\n",
        "        ax2.set_xticks(np.arange(0, self.number_of_epochs + 1, 5))\n",
        "        ax2.set_ylabel('Loss Value')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_title('Loss')\n",
        "        l2 = ax2.legend(loc=\"best\")\n",
        "        plt.savefig(str(traindatadir / \"lossplot\"))\n",
        "        # plt.show()\n",
        "\n",
        "    # =================================\n",
        "    # create and store the loss plot diagram\n",
        "    #\n",
        "    def plot(self, traindatadir, numepochs):\n",
        "        # plot the training loss and accuracy\n",
        "        N = numepochs\n",
        "        H = self.history\n",
        "        plt.style.use(\"ggplot\")\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
        "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "        plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "        plt.xlabel(\"Epoch #\")\n",
        "        plt.ylabel(\"Loss/Accuracy\")\n",
        "        plt.legend(loc=\"lower left\")\n",
        "        plt.savefig(traindatadir + \"lossplot\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ecgo4iWWVQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ============================================================================\n",
        "#\n",
        "class testMobileNetV2_MultiLabel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = 0\n",
        "        self.df_test = 0  # dataframe\n",
        "        self.model_data_name = \"\"\n",
        "        self.mlb_test = 0\n",
        "        self.width = 0\n",
        "        self.height = 0\n",
        "        self.root_dir_test = \"\"\n",
        "        self.resultstring = \"\"\n",
        "\n",
        "    def enumerate_test_images(self):\n",
        "        # parse the images directories and create the dataframe with images pathes and label lists\n",
        "        print(self.root_dir_test)\n",
        "        self.df_test = createDataFrame_fromDirectory(self.root_dir_test)\n",
        "        # Fit a MultiLabelBinarizer\n",
        "        self.mlb_test = MultiLabelBinarizer()\n",
        "        self.mlb_test.fit(self.df_test['tags'].values.tolist())\n",
        "        print(\"found this image classes in test data:\")\n",
        "        print(self.mlb_test.classes_)\n",
        "\n",
        "    def buildModel(self):\n",
        "        # load to continue training?\n",
        "        #\n",
        "        # even a compile is unnecessary\n",
        "        # https://www.tensorflow.org/tutorials/distribute/save_and_load\n",
        "        self.model = tf.keras.models.load_model(str(self.model_data_name))\n",
        "\n",
        "        # for inference:\n",
        "        #DEFAULT_FUNCTION_KEY = \"serving_default\"\n",
        "        #loaded = tf.saved_model.load(str(self.model_data_name))\n",
        "        #inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\n",
        "\n",
        "\n",
        "    # =================================\n",
        "    # test with test data\n",
        "    #\n",
        "    def testing(self):\n",
        "\n",
        "        # the model with unseen images\n",
        "        print(\"Testing with test_generator ...\")\n",
        "\n",
        "        test_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input)\n",
        "        test_generator = test_datagen.flow_from_dataframe(\n",
        "            dataframe=self.df_test,\n",
        "            directory='./',\n",
        "            x_col='imagepath',  # column names in the df\n",
        "            y_col='index',\n",
        "            class_mode='other',\n",
        "            target_size=(self.width, self.height),\n",
        "            color_mode=\"rgb\",\n",
        "            batch_size=1,   # each image exactly once\n",
        "            shuffle=False)  # test sequence in order\n",
        "\n",
        "        multilabel_test_generator = multilabel_flow_from_dataframe(test_generator, self.mlb_test, self.df_test)\n",
        "        STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
        "        test_generator.reset()\n",
        "        predictions = self.model.predict_generator(multilabel_test_generator,\n",
        "                                                   steps=STEP_SIZE_TEST,\n",
        "                                                   verbose=0)  # 0=silent, 1 = progress bar, 2 = one line per epoch\n",
        "\n",
        "        # predicted_class_indices = np.argmax(pred, axis=1)\n",
        "        # labels = test_generator.class_indices\n",
        "        # labels = dict((v, k) for k, v in labels.items())\n",
        "        # predictions = [labels[k] for k in predicted_class_indices]\n",
        "\n",
        "        filenames = test_generator.filenames  # contains each test files only once\n",
        "\n",
        "        top2_probabilities_list = []\n",
        "        number_of_correct_predictions = 0\n",
        "        for (filename, probabilities_list) in zip(filenames, predictions):\n",
        "            predictionstring = filename\n",
        "            sortet_propability_indexes = np.argsort(probabilities_list)[::-1]\n",
        "            propability_index_0 = sortet_propability_indexes[0]\n",
        "            propability_index_1 = sortet_propability_indexes[1]\n",
        "            label0 = self.mlb_test.classes_[propability_index_0]\n",
        "            label1 = self.mlb_test.classes_[propability_index_1]\n",
        "            if label0[0] == 'G':  # we want Red left and Green on right\n",
        "                x = label0\n",
        "                label0 = label1\n",
        "                label1 = x\n",
        "            # propability_percent_val_0 = probabilities_list[propability_index_0] * 100\n",
        "            # propability_percent_val_1 = probabilities_list[propability_index_1] * 100\n",
        "            top2_probabilities_list.append([label0, label1])\n",
        "\n",
        "            expected_label0 = os.path.split(os.path.dirname(filename))[-1].split('_')[0]\n",
        "            expected_label1 = os.path.split(os.path.dirname(filename))[-1].split('_')[1]\n",
        "            expected_angle0 = ''.join(i for i in expected_label0 if i.isdigit())\n",
        "            expected_angle1 = ''.join(i for i in expected_label1 if i.isdigit())\n",
        "            pred_angle0 = ''.join(i for i in label0 if i.isdigit())  # extract number\n",
        "            pred_angle1 = ''.join(i for i in label1 if i.isdigit())  # extract number\n",
        "\n",
        "            if (expected_angle0 == pred_angle0):\n",
        "                number_of_correct_predictions += 1\n",
        "            if (expected_angle1 == pred_angle1):\n",
        "                number_of_correct_predictions += 1\n",
        "\n",
        "            # predictionstring += \" {}: {:.2f}% , {}: {:.2f}%\".format(label0, propability_percent_val_0, label1, propability_percent_val_1)\n",
        "            # print(predictionstring)\n",
        "\n",
        "        all_predictions = 2 * len(filenames)\n",
        "        number_of_correct_predictions_percent = number_of_correct_predictions / (all_predictions) * 100\n",
        "        self.resultstring = \"correct predictions: {} of {} = {:.2f}%\".format(number_of_correct_predictions,\n",
        "                                                                             all_predictions,\n",
        "                                                                             number_of_correct_predictions_percent)\n",
        "        print(self.resultstring)\n",
        "\n",
        "        results = pd.DataFrame({\"Filename\": filenames, \"Predictions\": top2_probabilities_list})\n",
        "        results.to_csv(self.resultscsv_name, index=False)\n",
        "\n",
        "        print(\"Done testing. Results written to results.csv\")\n",
        "\n",
        "    # =================================\n",
        "    # The trained model can now be used to predict which class a new unseen\n",
        "    # image belongs to, by using model.predict(new_image).\n",
        "    #\n",
        "    def predict(self, img_path):\n",
        "        print(\"predicting: \" + img_path)\n",
        "        img = keras.preprocessing.image.load_img(img_path, target_size=(self.width, self.height))\n",
        "        image = keras.preprocessing.image.img_to_array(img)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = keras.applications.mobilenet.preprocess_input(image)\n",
        "        probabilities = self.model.predict(image)[0]\n",
        "        idxs = np.argsort(probabilities)[::-1][:2]\n",
        "        # print(\"len: \" + str(len(idxs)))\n",
        "\n",
        "        # loop over the indexes of the high confidence class labels\n",
        "        print(\"Prediction:\")\n",
        "        for (i, j) in enumerate(idxs):\n",
        "            # print the two labels for Red and Green thread:\n",
        "            label = \"{}: {:.2f}%\".format(self.mlb_test.classes_[j], probabilities[j] * 100)\n",
        "            print(label)\n",
        "\n",
        "        # show the probabilities for each of the individual labels\n",
        "        print(\"Details:\")\n",
        "        for (label, p) in zip(self.mlb_test.classes_, probabilities):\n",
        "            print(\"{}: {:.2f}%\".format(label, p * 100))\n",
        "\n",
        "    # ============================================================================\n",
        "    #\n",
        "    def predictAll(self, root_dir_test):\n",
        "        image_filename_list = self.df_test.iloc[:]['imagepath']\n",
        "        number_of_correct_predictions = 0\n",
        "        number_of_nearly_correct_predictions = 0\n",
        "        for imagefile in image_filename_list:\n",
        "            img = keras.preprocessing.image.load_img(imagefile, target_size=(self.width, self.height))\n",
        "            image = keras.preprocessing.image.img_to_array(img)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            image = keras.applications.mobilenet.preprocess_input(image)\n",
        "            # ****\n",
        "            probabilities_list = self.model.predict(image)[0]  # inference of single image\n",
        "            # ****\n",
        "            probability_index_green = np.argsort(probabilities_list[:7])[::-1][0]  # 0..6\n",
        "            probability_index_red   = np.argsort(probabilities_list[7:])[::-1][0]  # 7..13\n",
        "            label_green = self.mlb_test.classes_[probability_index_green]\n",
        "            label_red   = self.mlb_test.classes_[probability_index_red + 7]\n",
        "            if label_red[0] == 'G' or label_green[0] == 'R':  # we want Red in label_red and Green in label_green\n",
        "                print(\"unexpected label type: should not happen!\")\n",
        "\n",
        "            expected_label_red = os.path.split(os.path.dirname(imagefile))[-1].split('_')[0]  # RED\n",
        "            expected_label_green = os.path.split(os.path.dirname(imagefile))[-1].split('_')[1]  # GREEN\n",
        "\n",
        "            # RAND=999\n",
        "            label_red            = label_red.replace(\"RAND\", \"999\")\n",
        "            label_green          = label_green.replace(\"RAND\", \"999\")\n",
        "            expected_label_red   = expected_label_red.replace(\"RAND\", \"999\")\n",
        "            expected_label_green = expected_label_green.replace(\"RAND\", \"999\")\n",
        "\n",
        "            # extract number from labelname\n",
        "            expected_angle_red = int(''.join(i for i in expected_label_red if i.isdigit()))\n",
        "            expected_angle_green = int(''.join(i for i in expected_label_green if i.isdigit()))\n",
        "            pred_angle_red = int(''.join(i for i in label_red if i.isdigit()))\n",
        "            pred_angle_green = int(''.join(i for i in label_green if i.isdigit()))\n",
        "\n",
        "            tolerance = 16\n",
        "            if expected_angle_green == pred_angle_green:\n",
        "                number_of_correct_predictions += 1\n",
        "            elif (pred_angle_green >= (expected_angle_green - tolerance)) and (pred_angle_green <= (expected_angle_green + tolerance)):\n",
        "                number_of_nearly_correct_predictions += 1\n",
        "            else:\n",
        "                print(imagefile)\n",
        "                print(\"expected angle green:\", expected_angle_green, \"got:\", pred_angle_green)\n",
        "\n",
        "            if expected_angle_red == pred_angle_red:\n",
        "                number_of_correct_predictions += 1\n",
        "            elif (pred_angle_red >= (expected_angle_red - tolerance)) and (pred_angle_red <= (expected_angle_red + tolerance)):\n",
        "                number_of_nearly_correct_predictions += 1\n",
        "            else:\n",
        "                print(imagefile)\n",
        "                print(\"expected angle red:\", expected_angle_red, \"got:\", pred_angle_red)\n",
        "\n",
        "        all_predictions = 2 * len(image_filename_list)\n",
        "        number_of_correct_predictions_percent = number_of_correct_predictions / all_predictions * 100\n",
        "        self.resultstring = \"correct predictions: {} of {} = {:.2f}%\".format(number_of_correct_predictions,\n",
        "                                                                             all_predictions,\n",
        "                                                                             number_of_correct_predictions_percent)\n",
        "        print(\"predictAll: \" + self.resultstring)\n",
        "\n",
        "        number_of_nearly_correct_predictions += number_of_correct_predictions\n",
        "        number_of_nearly_correct_predictions_percent = number_of_nearly_correct_predictions / all_predictions * 100\n",
        "        self.resultstringnear = \"nearly correct: {} of {} = {:.2f}%\".format(number_of_nearly_correct_predictions,\n",
        "                                                                            all_predictions,\n",
        "                                                                            number_of_nearly_correct_predictions_percent)\n",
        "        print(\"predictAll: \" + self.resultstringnear)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCy-JePQXheo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MQ1hgloQrI-",
        "colab_type": "code",
        "outputId": "835b7234-f71e-4142-8613-3a707ffb550d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# ============================================================================\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    all_train_data_dir = pathlib.Path('traindir_' + current_time)\n",
        "\n",
        "    images_root_dir = pathlib.Path('imagesRedGreen2_1000_128x128')\n",
        "\n",
        "    doTrain = True\n",
        "    doTest = True\n",
        "\n",
        "    if doTrain:\n",
        "        pathlib.Path(all_train_data_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # hyper parameter:\n",
        "    width_height  = 128   # possible values: 96, 128, 160, 192, 224\n",
        "    learning_rate = 0.001 # start with\n",
        "    epochs        = 15\n",
        "    fixed_layers  = 0     # non-trainable\n",
        "    alpha         = 1.0   # possible values: 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n",
        "\n",
        "    if doTrain:\n",
        "        print(\"start training ...\")\n",
        "        cnn = trainMobileNetV2_MultiLabel()\n",
        "        cnn.root_dir_train = images_root_dir / 'train'\n",
        "        cnn.root_dir_valid = images_root_dir / 'valid'\n",
        "        cnn.root_dir_test = images_root_dir / 'valid'\n",
        "\n",
        "        cnn.plot_name = all_train_data_dir / 'model_plot.png'\n",
        "        cnn.model_data_name = all_train_data_dir / 'saved_model_state.model'\n",
        "        cnn.mlb_data_name = all_train_data_dir / 'mlb.pickle'\n",
        "        cnn.width = width_height  # possible values: 96, 128, 160, 192, 224\n",
        "        cnn.height = width_height\n",
        "\n",
        "        print(\"enumerating images ...\")\n",
        "        cnn.enumerate_training_images()\n",
        "\n",
        "        cnn.build_model(fixed_layers, learning_rate, alpha)\n",
        "\n",
        "        batch_size = 32\n",
        "        cnn.train(epochs, batch_size)\n",
        "        numclasses = len(cnn.classes_list)\n",
        "\n",
        "        # plot accuracy and loss and save diagram\n",
        "        cnn.plot2(all_train_data_dir)\n",
        "\n",
        "        print(\"training Done.\")\n",
        "\n",
        "    # ===================== testing =================================================\n",
        "    #\n",
        "    if doTest:\n",
        "        print(\"start testing ...\")\n",
        "\n",
        "        if not doTrain:\n",
        "            # use pretrained model:\n",
        "            all_train_data_dir = pathlib.Path('traindir_20191219_154722')\n",
        "\n",
        "        cnntest = testMobileNetV2_MultiLabel()\n",
        "        cnntest.width = width_height  # possible values: 96, 128, 160, 192, 224\n",
        "        cnntest.height = width_height\n",
        "\n",
        "        cnntest.model_data_name = all_train_data_dir / 'saved_model_state.model'\n",
        "        cnntest.mlb_data_name = all_train_data_dir / 'mlb.pickle'\n",
        "        cnntest.resultscsv_name = all_train_data_dir / 'results.csv'\n",
        "\n",
        "        cnntest.root_dir_test = images_root_dir / 'valid'\n",
        "\n",
        "        cnntest.enumerate_test_images()\n",
        "\n",
        "        cnntest.buildModel()\n",
        "\n",
        "        # img_path = './imagesRedGreen200/valid/RED15_GREEN345/telltale1_20_349.png'\n",
        "        # cnntest.predict(img_path)\n",
        "\n",
        "        cnntest.predictAll(all_train_data_dir)\n",
        "\n",
        "        cnntest.testing()\n",
        "\n",
        "        print(\"testing Done.\")\n",
        "\n",
        "    # document all training parameters:\n",
        "    with open(all_train_data_dir / \"trainingParameters.txt\", \"a+\") as text_file:\n",
        "        if doTrain:\n",
        "            print(\"======== Training and Testing =======\")\n",
        "        else:\n",
        "            print(\"======== Testing =======\")\n",
        "        print(f\"Training Params of today {current_time}\", file=text_file)\n",
        "        print(f\"Image witdth/height:     {width_height}\", file=text_file)\n",
        "        print(f\"Training data:           {str(images_root_dir)}\", file=text_file)\n",
        "        if doTrain:\n",
        "            print(f\"Number of fix layers:    {fixed_layers}\", file=text_file)\n",
        "            print(f\"Number of epochs:        {epochs}\", file=text_file)\n",
        "            print(f\"Number of categories:    {numclasses}\", file=text_file)\n",
        "            print(f\"learning_rate:           {learning_rate}\", file=text_file)\n",
        "            print(f\"Batch  size:             {batch_size}\", file=text_file)\n",
        "            print(f\"alpha:                   {alpha}\", file=text_file)\n",
        "            print(f\"nur ein einziges dense512 Layer\", file=text_file)\n",
        "            print(f\"keine vortrainierten Gewichte\", file=text_file)\n",
        "        print(f\"test results: {cnntest.resultstring}\", file=text_file)\n",
        "        print(f\"test results: {cnntest.resultstringnear}\", file=text_file)\n",
        "        text_file.close()\n",
        "\n",
        "    if IN_COLAB:\n",
        "        shutil.copytree(str(all_train_data_dir), '/content/drive/My Drive/' + str(all_train_data_dir))\n",
        "\n",
        "    print(\"All Done.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training ...\n",
            "enumerating images ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}